{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves to reproduce the experiments done in the main paper, and detailed in appendix B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "t.set_num_threads(8)\n",
    "import pandas as pd\n",
    "from train import train\n",
    "from models import Transformer, AoT\n",
    "from utils import generate_data, power_unif_law\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed=2222\n",
    "repetition=2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cells below to produce the data obtained in the folder Scaling laws. Experiments 1 to 4 are done exclusively on AoT, while experiment 5 involves general Transformers.\n",
    "\n",
    "**Experiment 1**: we test the scaling in the variable $H$ (or *para* in the notebook), which is the number of heads.\n",
    "Result: the scaling is found to be linear, as expected.\n",
    "\n",
    "**Experiment 2**: we test the scaling of the variable $d_{h}$, which is the dimension of each head.\n",
    "Result: the scaling is linear as expected.\n",
    "\n",
    "**Experiment 3**: we test the scaling of the variable $d$, which is the embedding dimension.\n",
    "Result: the scaling is linear by part, being separated at $d=d_{h}$. The second linear scale is noisy, meaning that this might be an optimization issue.\n",
    "\n",
    "**Experiment 4**: we test the scaling of $d=d_h$, when heads have the same dimension as the residual stream. (The data used in experiment 4 is the same as experiment 1.)\n",
    "Result: We find a scaling which is cubic. The scaling as expected to be at least quadratic, using the results from experiment 2 and 3. The scaling being cubic might suggest that experiment 3 had indeed optimization issues, which were uplifted by taking $d=d_h$.\n",
    "\n",
    "**Experiment 5**: we test the scaling of a Transformer with one attention head, and an MLP with varying width.\n",
    "Result: we find that the accuracy, by parameter, of both the AoT and the MLP-based Transformer are equivalent.\n",
    "\n",
    "**Experiement 6**: we test the scaling in the setting of Corollary 1, with $d=2$. We find that our solution in that case is not optimal as the observed scaling is greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Common hyper-parameters (exp 1-5). \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "N = 50\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1., 1., 1.]\n",
    "nb_tokens=[N, N, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "lr=1e-3\n",
    "epochs=10\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run d=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run H=1\n",
      "Run rep=0\n",
      "Run rep=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [01:03<06:22, 63.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run H=6\n",
      "Run rep=0\n",
      "Run rep=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [03:20<08:52, 106.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run H=11\n",
      "Run rep=0\n",
      "Run rep=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [08:25<13:09, 197.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run H=16\n",
      "Run rep=0\n",
      "Run rep=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [13:17<11:43, 234.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run H=21\n",
      "Run rep=0\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Experiment 1. Scaling laws on H and d=d_head. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Scaling parameters\n",
    "min_para=1\n",
    "max_para=31\n",
    "para_step=5\n",
    "min_d=3\n",
    "max_d=13\n",
    "d_step=1\n",
    "\n",
    "for i, d in enumerate(range(min_d, max_d+1, d_step)):\n",
    "    print(f\"Run d={d}\")\n",
    "    d_head=d \n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "\n",
    "    for para in tqdm(range(min_para, max_para+1, para_step)):\n",
    "        print(f\"Run H={para}\")\n",
    "        accuracy = 0\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            print(f\"Run rep={_}\")\n",
    "            model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_1_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 2. Scaling laws on d_head, with d!=d_head and H (=para) fixed. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "d = 10\n",
    "para = 20\n",
    "\n",
    "# Scaling parameters\n",
    "min_d_head = 1\n",
    "max_d_head = d\n",
    "d_head_step = 1\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "for d_head in tqdm(range(min_d_head, max_d_head+1, d_head_step)):\n",
    "    accuracy = 0\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "        \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 3. Scaling laws on  d, with d_head and H (=para) fixed. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "d_head = 10\n",
    "para = 20\n",
    "\n",
    "# Scaling parameters\n",
    "min_d = 5\n",
    "max_d = 15\n",
    "d_step = 1\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "for d in tqdm(range(min_d, max_d+1, d_step)):\n",
    "    accuracy = 0\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 5. Scaling laws on the width of Transformer using MLPs. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "para = 1\n",
    "\n",
    "for d, exp_num in zip([5, 7, 10, 13], [2, 4, 7, 10]):\n",
    "    d_head = d\n",
    "    min_width = 2*d*(1-1)\n",
    "    max_width = 2*d*(26-1)\n",
    "    step = 2*d*5\n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "    for width in tqdm(range(min_width, max_width+1, step)):\n",
    "        accuracy = 0\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = Transformer(d, N, nb_layers, width, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "            print(accuracy)\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "        width_list.append(width)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_5_{exp_num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 6. Scaling laws on H with N=10, d=2, d_head=5. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "N = 10\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1., 1., 1.]\n",
    "nb_tokens=[N, N, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "lr=1e-3\n",
    "epochs=15\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "# Scaling parameters\n",
    "min_para=1\n",
    "max_para=21\n",
    "para_step=4\n",
    "d=2\n",
    "d_head=5\n",
    "d_step=1\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "\n",
    "for para in tqdm(range(min_para, max_para+1, para_step)):\n",
    "    accuracy = 0\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional experiments: Large dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Common hyper-parameters. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "N = 200\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1., 1., 1.]\n",
    "nb_tokens=[N//2, N//2, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "lr=1e-3\n",
    "epochs=10\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 1. Scaling laws on H and d=d_head. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Scaling parameters\n",
    "d = 50\n",
    "d_head=d \n",
    "para_list=[1, 3, 5, 7, 9, 11]\n",
    "\n",
    "mean_accuracy = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "\n",
    "for para in tqdm(para_list):\n",
    "    accuracy = 0.\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_1_{7}_dim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 2. Scaling laws on d_head, with d!=d_head and H (=para) fixed. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Scaling parameters\n",
    "d = 50\n",
    "para = 8\n",
    "d_head_list = [1, 10, 20, 30, 40, 50]\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "for d_head in tqdm(d_head_list):\n",
    "    accuracy = 0.\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "        \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_2_dim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 5. Scaling laws on the width of Transformer using MLPs. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "for d, exp_num in zip([40, 50, 60], [4, 7, 10]):\n",
    "    para = 1\n",
    "    d_head = d\n",
    "    min_width = 2*d*(1-1)\n",
    "    max_width = 2*d*(11-1)\n",
    "    step = 2*d*2\n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "    for width in tqdm(range(min_width, max_width+1, step)):\n",
    "        accuracy = 0.\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = Transformer(d, N, nb_layers, width, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "        width_list.append(width)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_5_{exp_num}_dim.csv', index=False)\n",
    "\n",
    "\n",
    "for d, exp_num in zip([40, 60], [4, 10]):\n",
    "    d_head = d\n",
    "    min_para = 1\n",
    "    max_para = 11\n",
    "    step = 2\n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "    for para in tqdm(range(min_para, max_para+1, step)):\n",
    "        accuracy = 0.\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "        width_list.append(width)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_1_{exp_num}_dim.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional experiments: Large depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Common hyper-parameters. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "N = 50\n",
    "nb_layers = 5 # Depth of the network\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1., 1., 1.]\n",
    "nb_tokens=[N, N, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=1000\n",
    "lr=5e-4\n",
    "epochs=10\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 1. Scaling laws on H with fixed d=d_head. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Scaling parameters\n",
    "d = 10\n",
    "d_head=d \n",
    "para_list=[1, 6, 11, 16, 21]\n",
    "\n",
    "mean_accuracy=[]\n",
    "N_list=[]\n",
    "d_list=[]\n",
    "d_head_list=[]\n",
    "\n",
    "for para in tqdm(para_list):\n",
    "    accuracy = 0.\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_1_{7}_depth.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 2. Scaling laws on d_head, with d!=d_head and H (=para) fixed. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "d=10\n",
    "para=21\n",
    "d_head_list=[1, 3, 5, 7, 10]\n",
    "\n",
    "# Scaling parameters\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "for d_head in tqdm(d_head_list):\n",
    "    accuracy = 0.\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "        \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_2_depth.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 5. Scaling laws on the width of Transformer using MLPs. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "for d, exp_num in zip([7, 10, 13], [4, 7, 10]):\n",
    "    para = 1\n",
    "    d_head = d\n",
    "    min_width = 2*d*(1-1)\n",
    "    max_width = 2*d*(21-1)\n",
    "    step = 2*d*5\n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "    for width in tqdm(range(min_width, max_width+1, step)):\n",
    "        accuracy = 0.\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = Transformer(d, N, nb_layers, width, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "        width_list.append(width)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_5_{exp_num}_depth.csv', index=False)\n",
    "\n",
    "\n",
    "for d, exp_num in zip([7, 13], [4, 10]):\n",
    "    d_head = d\n",
    "    min_para = 1\n",
    "    max_para = 21\n",
    "    step = 5\n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "    for para in tqdm(range(min_para, max_para+1, step)):\n",
    "        accuracy = 0.\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "        width_list.append(width)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_1_{exp_num}_depth.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
