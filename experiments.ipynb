{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves to reproduce the experiments done in the main paper, and detailed in appendix B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import pandas as pd\n",
    "from train import train\n",
    "from models import Transformer, AoT\n",
    "from utils import generate_data, power_unif_law\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed=2222\n",
    "repetition=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cells below to produce the data obtained in the folder Scaling laws. Experiments 1 to 4 and 6 are done exclusively on AoT, while experiment 5 involves general Transformers.\n",
    "\n",
    "**Experiment 1**: we test the scaling in the variable $H$ (or *para* in the notebook), which is the number of heads.\\\n",
    "Result: the accuracy scaling law is linear in $H$.\n",
    "\n",
    "**Experiment 2**: we test the scaling of the variable $d_{h}$, which is the dimension of each head.\\\n",
    "Result: the accuracy scaling law is quadratic in $d_h$.\n",
    "\n",
    "**Experiment 3**: we test the scaling of the variable $d$, which is the embedding dimension, while maintaining $d_h$ constant.\\\n",
    "Result: the accuracy scaling law is linear by part for $d\\leq d_{h}$ and constant afterward.\n",
    "\n",
    "**Experiment 4**: we test the scaling of $d$, when heads have the same dimension as the residual stream. (The data used in experiment 4 is the same as experiment 1.)\\\n",
    "Result: we find a scaling which is cubic, which is expected using the results from experiment 2 and 3.\n",
    "\n",
    "**Experiment 5**: we test the scaling of a Transformer with one attention head, and an MLP with varying width.\\\n",
    "Result: we find that the accuracy, by parameter, of the AoT is smaller than that of the MLP-based Transformer. However, the MLP-based Transformer is harder to optimize, meaning that for the same computation power, AoT can offer a greater accuracywith finite compute.\n",
    "\n",
    "**Experiment 6**: we test the scaling in the setting of Corollary 1, with $d=2$.\\\n",
    "Result: we find that our solution in that case is not optimal as the observed scaling is greater by a factor 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Common hyper-parameters (exp 1-4). \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "N = 50\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1., 1., 1.]\n",
    "nb_tokens=[N, N, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**8\n",
    "num_batch=2**6\n",
    "lr=1e-1\n",
    "lr_low=lr*5e-2\n",
    "epochs=2**6\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "device=\"cpu\" # Metal is much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 1. Scaling laws on H and d=d_head. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Scaling parameters\n",
    "min_para=1\n",
    "max_para=31\n",
    "para_step=5\n",
    "min_d=3\n",
    "max_d=13\n",
    "d_step=1\n",
    "\n",
    "for d in range(min_d, max_d+1, d_step):\n",
    "    print(f\"Run d={d}\")\n",
    "    d_head=d \n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "\n",
    "    for para in tqdm(range(min_para, max_para+1, para_step)):\n",
    "        print(f\"Run H={para}\")\n",
    "        accuracy = 0\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            print(f\"Run rep={_}\")\n",
    "            model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_1_{d}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 2. Scaling laws on d_head, with d!=d_head and H (=para) fixed. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "d = 10\n",
    "para = 20\n",
    "\n",
    "# Scaling parameters\n",
    "min_d_head = 1\n",
    "max_d_head = d\n",
    "d_head_step = 1\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "for d_head in tqdm(range(min_d_head, max_d_head+1, d_head_step)):\n",
    "    accuracy = 0\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "        \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 3. Scaling laws on  d, with d_head and H (=para) fixed. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "d_head = 10\n",
    "para = 20\n",
    "\n",
    "# Scaling parameters\n",
    "min_d = 5\n",
    "max_d = 15\n",
    "d_step = 1\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "for d in tqdm(range(min_d, max_d+1, d_step)):\n",
    "    accuracy = 0\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 5. Scaling laws on the width of Transformer using MLPs. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "N = 70\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1., 1., 1.]\n",
    "nb_tokens=[N, N, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=2**8\n",
    "lr=1e-1\n",
    "epochs=2**6\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "device=\"cpu\" # Metal is much slower\n",
    "\n",
    "for d in [7, 10, 13]: # Train regular Transformers\n",
    "    d_head = d\n",
    "    min_para = 1\n",
    "    max_para = 31\n",
    "    step_para = 5\n",
    "    lr_low=lr*0.01\n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "    for para in tqdm(range(min_para, max_para+1, step_para)):\n",
    "        width = 2*d*(para-1)\n",
    "        accuracy = 0\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = Transformer(d, N, nb_layers, width, 1, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "        width_list.append(width)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_5_{d}_mlp.csv', index=False)\n",
    "\n",
    "for d in [7, 10, 13]: # Train AoT\n",
    "    d_head = d\n",
    "    min_para = 1\n",
    "    max_para = 31\n",
    "    step_para = 5\n",
    "    lr_low=lr*5e-2\n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "    for para in tqdm(range(min_para, max_para+1, step_para)):\n",
    "        width = 2*d*(para-1)\n",
    "        accuracy = 0\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = Transformer(d, N, nb_layers, 0, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "        width_list.append(width)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_5_{d}_att.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 6. Scaling laws on H with N=10, d=2, d_head=d. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "repetition=20\n",
    "\n",
    "# Model parameters.\n",
    "N = 10\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1., 1., 1.]\n",
    "nb_tokens=[N, N, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**8\n",
    "num_batch=2**6\n",
    "lr=1e-1\n",
    "lr_low=lr*5e-2\n",
    "epochs=2**6\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)\n",
    "\n",
    "\n",
    "# Scaling parameters\n",
    "min_para=1\n",
    "max_para=21\n",
    "para_step=4\n",
    "d=2\n",
    "d_head=d\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "d_head_list = []\n",
    "\n",
    "for para in tqdm(range(min_para, max_para+1, para_step)):\n",
    "    accuracy = 0\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    d_head_list.append(d_head)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_6.csv', index=False)\n",
    "\n",
    "repetition=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional experiments: Large dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this follow-up experiment, we reproduce experiments 1, 2 and 5 in dimension $d=50$ and with $N=200$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Common hyper-parameters. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "N = 200\n",
    "nb_layers = 1\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1., 1., 1.]\n",
    "nb_tokens=[N, N, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**10\n",
    "num_batch=2**6\n",
    "lr=1e-1\n",
    "lr_low=lr*5e-2\n",
    "epochs=2**6\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 1. Scaling laws on H and d=d_head. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Scaling parameters\n",
    "para_list=[1, 3, 5, 7, 9, 11]\n",
    "\n",
    "for d in [40, 50, 60]:\n",
    "    d_head=d \n",
    "\n",
    "    mean_accuracy = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "\n",
    "    for para in tqdm(para_list):\n",
    "        accuracy = 0.\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_1_{d}_dim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 2. Scaling laws on d_head, with d!=d_head and H (=para) fixed. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Scaling parameters\n",
    "d = 50\n",
    "para = 11\n",
    "d_head_list = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "for d_head in tqdm(d_head_list):\n",
    "    accuracy = 0.\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "        \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_2_dim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 5. Scaling laws on the width of Transformer using MLPs. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "lr_low=lr*0.01\n",
    "\n",
    "for d in [40, 50, 60]:\n",
    "    para = 1\n",
    "    d_head = d\n",
    "    min_width = 2*d*(1-1)\n",
    "    max_width = 2*d*(11-1)\n",
    "    step = 2*d*2\n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "    for width in tqdm(range(min_width, max_width+1, step)):\n",
    "        accuracy = 0.\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = Transformer(d, N, nb_layers, width, 1, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "        width_list.append(width)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_5_{d}_dim.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional experiments: Large depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this follow-up experiment, we reproduce experiments 1, 2 and 5 in dimension $d=10$ and with 3 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Common hyper-parameters. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "N = 70\n",
    "nb_layers = 3 # Depth of the network\n",
    "nb_head = 1\n",
    "n_gram = 3\n",
    "context_window = n_gram\n",
    "\n",
    "# Distribution parameters.\n",
    "alphas = [1., 1., 1.]\n",
    "nb_tokens=[N, N, 1]\n",
    "pi = power_unif_law(alphas, nb_tokens, N)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size=2**9\n",
    "num_batch=2**7\n",
    "lr=5e-2\n",
    "lr_low=lr\n",
    "epochs=2**6\n",
    "Data = generate_data(batch_size=batch_size, num_batch=num_batch, pi=pi, context_window=context_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 1. Scaling laws on H with fixed d=d_head. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Scaling parameters\n",
    "para_list=[1, 5, 9, 13, 17, 21]\n",
    "\n",
    "for d in [7, 10, 13]:\n",
    "    d_head=d\n",
    "    mean_accuracy=[]\n",
    "    N_list=[]\n",
    "    d_list=[]\n",
    "    d_head_list=[]\n",
    "\n",
    "    for para in tqdm(para_list):\n",
    "        accuracy = 0.\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_1_{d}_depth.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 2. Scaling laws on d_head, with d!=d_head and H (=para) fixed. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "# Model parameters.\n",
    "d=10\n",
    "para=21\n",
    "d_head_list=[i for i in range(1, 10+1)]\n",
    "\n",
    "# Scaling parameters\n",
    "mean_accuracy = []\n",
    "para_list = []\n",
    "N_list = []\n",
    "d_list = []\n",
    "for d_head in tqdm(d_head_list):\n",
    "    accuracy = 0.\n",
    "\n",
    "    for _ in range(repetition):\n",
    "        model = AoT(d, N, nb_layers, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "        dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "        acc = sum(dict['Acc'][-101:-1])/100\n",
    "        \n",
    "        accuracy += acc\n",
    "\n",
    "    mean_accuracy.append(accuracy/repetition)\n",
    "    N_list.append(N)\n",
    "    d_list.append(d)\n",
    "    para_list.append(para)\n",
    "\n",
    "results = {\n",
    "    'acc': mean_accuracy,\n",
    "    'para': para_list,\n",
    "    'N': N_list,\n",
    "    'd': d_list,\n",
    "    'd_head': d_head_list,\n",
    "}\n",
    "\n",
    "# We save the results as a dataframe.\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'Scaling laws/Data_exp_2_depth.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment 5. Scaling laws on the width of Transformer using MLPs. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "for d in [7, 10, 13]:\n",
    "    para = 1\n",
    "    d_head = d\n",
    "    min_width = 2*d*(1-1)\n",
    "    max_width = 2*d*(21-1)\n",
    "    step = 2*d*4\n",
    "\n",
    "    mean_accuracy = []\n",
    "    para_list = []\n",
    "    N_list = []\n",
    "    d_list = []\n",
    "    d_head_list = []\n",
    "    width_list = []\n",
    "    for width in tqdm(range(min_width, max_width+1, step)):\n",
    "        accuracy = 0.\n",
    "\n",
    "        for _ in range(repetition):\n",
    "            model = Transformer(d, N, nb_layers, width, para, d_head, nb_head, context_window, pi)\n",
    "\n",
    "            dict = train(model, Data, epochs, lr=lr, next_token=True, lr_low=lr_low)\n",
    "            acc = sum(dict['Acc'][-101:-1])/100\n",
    "            \n",
    "            accuracy += acc\n",
    "\n",
    "        mean_accuracy.append(accuracy/repetition)\n",
    "        N_list.append(N)\n",
    "        d_list.append(d)\n",
    "        d_head_list.append(d_head)\n",
    "        para_list.append(para)\n",
    "        width_list.append(width)\n",
    "\n",
    "    results = {\n",
    "        'acc': mean_accuracy,\n",
    "        'para': para_list,\n",
    "        'N': N_list,\n",
    "        'd': d_list,\n",
    "        'd_head': d_head_list,\n",
    "        'width': width_list,\n",
    "    }\n",
    "\n",
    "    # We save the results as a dataframe.\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'Scaling laws/Data_exp_5_{d}_depth.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
